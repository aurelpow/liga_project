{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63eb7ceb",
   "metadata": {},
   "source": [
    "# LIGA HISTORICAL DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12ac74a",
   "metadata": {},
   "source": [
    "## Web scraping to get La Liga data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "9d5a1120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "## Creating variables : \n",
    "SEASONS = list(range(2020, 2023))#Select the NBA season(s) to scrape\n",
    "STANDINGS_DIR = r'C:\\Users\\aureb\\OneDrive - Sport-Data\\Documents\\COURS\\DATABIRD\\Football_PROJECT\\data\\standings' #Final destination for the seasons .html files\n",
    "GAMES_DIR = r'C:\\Users\\aureb\\OneDrive - Sport-Data\\Documents\\COURS\\DATABIRD\\Football_PROJECT\\data\\games' # Final destination for the games .html files\n",
    "\n",
    "def scrape_games_html(season):\n",
    "    url = f\"https://fbref.com/en/comps/12/{season}-{season+1}/schedule/La-Liga-Scores-and-Fixtures\"# Formatted string with the variable seaso\n",
    "    # Send a request to the website\n",
    "    response = requests.get(url)\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Extract information based on the HTML structure\n",
    "        # Modify this part based on the structure of the website\n",
    "        links = soup.find_all(\"a\", href=lambda href: href and href.startswith(\"/en/matches/\") and href.endswith(\"La-Liga\") )\n",
    "        game_pages = [f\"https://fbref.com/{l['href']}\" for l in links]\n",
    "\n",
    "        # Create a folder to save HTML files if it doesn't exist\n",
    "        folder_path = GAMES_DIR\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        # Iterate through the game pages and save HTML content\n",
    "        for page_url in tqdm(game_pages):\n",
    "            # Define a regular expression pattern to capture the desired portion\n",
    "            pattern = r\"/matches/([^/]+/.+)$\"\n",
    "            # Use re.search to find the match\n",
    "            match = re.search(pattern, page_url)\n",
    "            match = match.group(1)\n",
    "            # Replace '/' with '-'\n",
    "            match = match.replace('/', '-')\n",
    "            save_path = os.path.join(folder_path, f'{match}.html')\n",
    "            if os.path.exists(save_path):\n",
    "                    continue\n",
    "            response = requests.get(page_url)\n",
    "            if response.status_code == 200:\n",
    "                # Save the HTML content in a file\n",
    "                with open(os.path.join(folder_path, f'{match}.html'), 'w', encoding='utf-8') as file:\n",
    "                    file.write(response.text)\n",
    "                print(f\"Saved HTML for {page_url}\")\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                print(f\"Failed to retrieve HTML for {page_url}\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve main page. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a6b04cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb0f474c5ff04642829da0fe80edcada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d483f7dc224e0c93ce83f3c9c700d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca07c94e14ec4df48380e10339c12a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Executing the function to scrape all the games :\n",
    "for season in SEASONS:\n",
    "     scrape_games_html(season)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04946eb9",
   "metadata": {},
   "source": [
    "## Create a DataFrame with pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8335b547",
   "metadata": {},
   "source": [
    "### Function 1 :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "3697d10c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a function to \"clean\" the html code of the game page before extracting the tables we need\n",
    "def parse_html(html_file):\n",
    "    with open(html_file, 'r', encoding='utf-8') as f:  # Specify the encoding here\n",
    "        html = f.read()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        [s.decompose() for s in soup.select(\"tr.over_header\")]  # Remove the \"basic Box score Stats\" header\n",
    "        [s.decompose() for s in soup.select(\"tr.thead\")]  # Remove reserves row\n",
    "        return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb8a271",
   "metadata": {},
   "source": [
    "### Function 2 : Get the basic stats from each html file saved into the local folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "ebb47acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basic_stats(html_file):\n",
    "    soup = parse_html(html_file)#Executing the code with the function created\n",
    "    # Extract the score and xG :\n",
    "    score_list = []\n",
    "    xG_list = []\n",
    "    for item in soup.find_all('div', {'class': 'scores'}):\n",
    "        xG = item.text.strip()[-3:]\n",
    "        score = item.text.strip()[:-3]\n",
    "        score_list.append(score)\n",
    "        xG_list.append(xG)\n",
    "    #Exctract the date: \n",
    "    date_span = soup.find('span', {'class': 'venuetime'})\n",
    "    date = date_span['data-venue-date']\n",
    "    #Exctract the team names: \n",
    "    team_stats_div = soup.find('div', {'id': 'team_stats'})\n",
    "    team_home =team_stats_div.find('span',{ 'class' : 'teamandlogo', 'style' : 'padding-right: 10px'}).text.strip()\n",
    "    team_away = team_stats_div.find('span',{ 'class' : 'teamandlogo', 'style' : 'padding-left: 10px'}).text.strip()\n",
    "    # Extract the Team stats percentage : \n",
    "    basic_stats_values = [strong_tag.text for strong_tag in team_stats_div.find_all('strong')]\n",
    "    cleaned_stats_values = [percentage.strip() for percentage in basic_stats_values]\n",
    "    # Function to replace percentage symbol with NaN\n",
    "    def replace_percentage_with_nan(value):\n",
    "        if value == '%':\n",
    "            return np.nan\n",
    "        else:\n",
    "            return value\n",
    "    # Function to check if the category we want is in the html file :\n",
    "    def check_for_th(soup, text_name):\n",
    "        def match_th(tag):\n",
    "            return tag.name == 'th' and tag.get('colspan') == '2' and tag.get_text(strip=True) == text_name\n",
    "        target_th = soup.find(match_th)\n",
    "        return \"YES\" if target_th else \"NO\"\n",
    "    # Now we can scrap the percentage stats : \n",
    "    possession = [replace_percentage_with_nan(value) for value in cleaned_stats_values[:2]] if check_for_th(soup, 'Possession') == \"YES\" else [np.nan, np.nan]\n",
    "    passing_accuracy = [replace_percentage_with_nan(value) for value in cleaned_stats_values[2:4]] if check_for_th(soup, 'Passing Accuracy') == \"YES\" else [np.nan, np.nan]\n",
    "    shots_on_target = [replace_percentage_with_nan(value) for value in cleaned_stats_values[4:6]] if check_for_th(soup, 'Shots on Target') == \"YES\" else [np.nan, np.nan]\n",
    "    saves = [replace_percentage_with_nan(value) for value in cleaned_stats_values[6:8]] if check_for_th(soup, 'Saves') == \"YES\" else [np.nan, np.nan]\n",
    "    # Extract the number of yellow and red cards :\n",
    "    yellow_cards_list = []\n",
    "    red_cards_list = []\n",
    "    for x in range(0,2):\n",
    "        card = soup.find_all('div', {'class': 'cards'})\n",
    "        yellow_cards = card[x].select('div.cards span.yellow_card')\n",
    "        red_cards = card[x].select('div.cards span.red_card, div.cards span.yellow_red_card')\n",
    "        num_yellow_cards = len(yellow_cards)\n",
    "        num_red_cards = len(red_cards)\n",
    "        yellow_cards_list.append(num_yellow_cards)\n",
    "        red_cards_list.append(num_red_cards)\n",
    "        # Use regular expression to extract the alphanumeric string between the last '\\\\' and the first '-'\n",
    "        match = re.search(r'\\\\([a-zA-Z0-9]+)-', html_file)\n",
    "        if match:\n",
    "            game_id = match.group(1)\n",
    "        else:\n",
    "            print(\"Pattern not found in the file path.\")\n",
    "    # Create a DataFrame\n",
    "    data = {\n",
    "        'Date' : date,\n",
    "        'game_id' : game_id,\n",
    "        'Team': [team_home, team_away],\n",
    "        'Goals' : score_list,\n",
    "        'xG' : xG_list,\n",
    "        'Possession': possession,\n",
    "        'Passing Accuracy': passing_accuracy,\n",
    "        'Shots on Target': shots_on_target,\n",
    "        'Saves': saves,\n",
    "        'Yellow_cards' :yellow_cards_list,\n",
    "        'Red_cards' :red_cards_list, \n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    # Cleaning the DATE : string to date format\n",
    "    df['Date'] = pd.to_datetime(df['Date']).dt.date\n",
    "    #Adding a new Season column that we can separate the data per season if we want to\n",
    "    # Define a function to calculate the year based on the date\n",
    "    def calculate_year(row):\n",
    "        if row['Date'].month >= 8:\n",
    "            return row['Date'].year + 1\n",
    "        else:\n",
    "            return row['Date'].year\n",
    "    #Apply the calculate_year function to the new column to get the Season : \n",
    "    df[\"Season\"] = df.apply(calculate_year, axis=1)\n",
    "    #Cleaning the columns were the figures come with the \"%\"\n",
    "    # Function to convert percentage columns to float\n",
    "    def convert_percentage_column(column_name, dataframe):\n",
    "        dataframe[column_name] = dataframe[column_name].apply(lambda x: float(x.rstrip('%')) if isinstance(x, str) else x)\n",
    "        return dataframe\n",
    "    # Apply the function to all percentage columns\n",
    "    percentage_columns = ['Goals', 'xG', 'Possession', 'Passing Accuracy', 'Shots on Target', 'Saves']\n",
    "    for column in percentage_columns:\n",
    "        convert_percentage_column(column, df)\n",
    "    # Convert the 'Goals' column to integers\n",
    "    df['Goals'] = df['Goals'].astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d37e740",
   "metadata": {},
   "source": [
    "### Function 3 : Get the full team stats from each html file saved into the local folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "76db6830",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_extra_stats(html_file):\n",
    "    soup = parse_html(html_file)#Executing the code with the function created\n",
    "    #Create empty lists to stock the data we will crap \n",
    "    home_stats_extra_list = []\n",
    "    away_stats_extra_list = []\n",
    "    column_headers = ['Fouls', 'Corners', 'Crosses', 'Touches','Tackles' ,'Interceptions' ,\n",
    "              'Aerials_Won','Clearances','Offsides','Goal Kicks','Throw Ins','Long Balls' ] #headers for the final db\n",
    "    \n",
    "    extra_stats = soup.find('div', {'id': 'team_stats_extra'})\n",
    "    for item in extra_stats.find_all('div'):\n",
    "        current_teams = [team.strip() for team in item.text.strip().split(' ') if team]\n",
    "        value = item.text.strip()\n",
    "        numeric_values = [int(val) for val in re.findall(r'\\d+', value)]\n",
    "        if len(numeric_values) == 8:\n",
    "            home_stats = [numeric_values[0] , numeric_values[2], numeric_values[4], numeric_values[6]]\n",
    "            away_stats = [numeric_values[1] , numeric_values[3], numeric_values[5], numeric_values[7]]\n",
    "            home_stats_extra_list.append(home_stats[:4])\n",
    "            away_stats_extra_list.append(away_stats[:4])\n",
    "\n",
    "    # Flatten the list of lists\n",
    "    flattened_home_stats = [item for sublist in home_stats_extra_list for item in sublist]\n",
    "    flattened_away_stats = [item for sublist in away_stats_extra_list for item in sublist]\n",
    "\n",
    "\n",
    "    # Create DataFrame\n",
    "    data = [flattened_home_stats, flattened_away_stats]\n",
    "    df = pd.DataFrame(data, columns=column_headers)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00905df4",
   "metadata": {},
   "source": [
    "After created the functions, we can execute them into a loop going into all html files we scrapped before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "d8a2b89a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3679ed2c50854df9a1a1ca1c761d2863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "GAMES_DIR = r'C:\\Users\\aureb\\OneDrive - Sport-Data\\Documents\\COURS\\DATABIRD\\Football_PROJECT\\data\\games' # Final destination for the games .html files\n",
    "games_html = os.listdir(GAMES_DIR)\n",
    "games_html = [os.path.join(GAMES_DIR, f) for f in games_html if f.endswith(\".html\")]\n",
    "# Initialize an empty DataFrame\n",
    "all_games_df = pd.DataFrame()\n",
    "# Assuming games_html is a list containing HTML content for each game \n",
    "for game in tqdm(games_html) : \n",
    "    basic_stats = get_basic_stats(game) \n",
    "    extra_stats = get_extra_stats(game)  \n",
    "    # Concatenate basic_stats and extra_stats along the columns axis\n",
    "    result_df = pd.concat([basic_stats, extra_stats], axis=1)\n",
    "     # Concatenate the current game's DataFrame to the main DataFrame\n",
    "    all_games_df = pd.concat([all_games_df, result_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895a815d",
   "metadata": {},
   "source": [
    "# # Write database into a google sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "99aad369",
   "metadata": {},
   "outputs": [],
   "source": [
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import gspread\n",
    "## Identifying with google API to write the final database to a google sheet : \n",
    "    # Specify the path to your credentials JSON file\n",
    "json_keyfile =  \"C:/Users/aureb/OneDrive - Sport-Data/Documents/COURS/DATABIRD/PROJECT/imposing-bee-389610-823a1fac476d.json\"\n",
    "    # Define the scope\n",
    "scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
    "    # Authenticate using the credentials\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name(json_keyfile, scope)\n",
    "client = gspread.authorize(creds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "7db565ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function who will write the data after clearing the sheet\n",
    "def write_google_sheet(client,url,df_to_write):\n",
    "   # Open the Google Sheets document by URL\n",
    "   spreadsheet = client.open_by_url(url)\n",
    "   # Select the worksheet to which you want to write the data\n",
    "   worksheet = spreadsheet.get_worksheet(0)  # Get the first worksheet (index 0)\n",
    "   # Clear the specified range\n",
    "   worksheet.clear()\n",
    "    # Get the column names from the DataFrame\n",
    "   column_names = df_to_write.columns.tolist()\n",
    "    # Insert the column names as the first row in the worksheet\n",
    "   worksheet.insert_rows([column_names], 1)\n",
    "    # Export the DataFrame data to Google Sheets starting from the second row (row 2)\n",
    "   data_to_insert = df_to_write.values.tolist()\n",
    "   worksheet.insert_rows(data_to_insert, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "336117a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To be able to write into a google sheet, we need to fix some columns\n",
    "all_games_df['Date'] = all_games_df['Date'].astype(str) # Date from date format to string\n",
    "all_games_df = all_games_df.fillna('') # Replace all Nan values by nothing\n",
    "url_historical_data_teams_sheet = 'https://docs.google.com/spreadsheets/d/1hZMW4q_aJ17NLa8gCvXpoQ5YB6y-ftq3yW3-yGrG6l0/edit#gid=0'\n",
    "write_google_sheet(client,url_historical_data_teams_sheet,all_games_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
